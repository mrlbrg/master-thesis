\chapter{Related Work}
\label{chap:related-work}
In this chapter, we review related work on write-optimized data structures, focusing on B-tree variants and alternatives and compression techniques to reduce write amplification.
For each data structure, we discuss its design, how it addresses write amplification, and its trade-offs in terms of read performance, concurrency and applicability.
We outline the gap in existing work that we aim to address in this thesis.
In \autoref{sec:related-work-lsmt}, we discuss \ac{LSMT}, a popular write-optimized data structure alternative to B-trees.
In \autoref{sec:related-work-bepsilontrees}, we review B$^\epsilon$-trees, a write-optimized B-tree variant.
In \autoref{sec:related-work-bwtrees}, we discuss the Bw-tree, another B-tree variant optimized for modern hardware.
In \autoref{sec:related-work-wo-btrees}, we review Graefe's Write-Optimized B-trees, an approach to improve write performance in B-trees by batching page writes.
In \autoref{sec:related-work-fineline}, we discuss FineLine, a storage manager that decouples in-memory data structures from their disk representation.
Finally, in \autoref{sec:related-work-compression}, we review a hardware-based approach to reduce write amplification in B-trees using transparent compression.

\section{Log-Structured-Merge-Trees}
\label{sec:related-work-lsmt}
\ac{LSMT} \cite{oneil1996log} are a popular alternative index data structure to B-trees for write-heavy workloads.
They are increasingly used in key-value stores such as RocksDB at Meta \cite{rocksdb} or BigTable at Google \cite{chang2008bigtable}.

\inlinesection{Basic Structure.}
\ac{LSMT} consist of two main components: an in-memory component and a disk-based component.
The in-memory component is typically implemented as a balanced tree such as a red-black tree, called a MemTable.
The MemTable accepts and applies updates in memory.
Once it is full, it is flushed to disk as a sorted, immutable run in a file called SSTable.
Over time, multiple runs accumulate on disk.
Since those runs may have overlapping key ranges, lookups need to check both memory and multiple disk files to find a key.

To limit the number of runs on disk and improve lookup performance, \ac{LSMT} organize runs into multiple levels, where each level is larger and more data is sorted than in the previous one.
When a level reaches its size limit, it triggers a compaction process to sort-merge runs into the next level, retaining only the latest version of each key.
As a result, higher levels contain more recent data with several, smaller files with overlapping key ranges while lower levels contain few, large files with non-overlapping key ranges.
Since runs are immutable, each compaction generates new files for the merged runs.
Outdated files are deleted by a garbage collector \cite{sarkar2022lsmt}.

% TODO: Figure of LSMT maybe

\inlinesection{High Write Performance.}
B-trees maintain a fully sorted view of the data and update this view in-place.
In contrast, \ac{LSMT} update out-of-place in a sequential, log-structured manner by buffering updates in memory and flushing them to external storage in large, sorted batches, enabling high write throughput.
The excellent write performance of an \ac{LSMT} makes this data structure suitable for write-heavy workloads, such as time-series data or logging systems.

% Cloud Scenario
% Also, the immutability of files in \ac{LSMT} aligns well with that of objects in remote storage like S3.
% This makes \ac{LSMT} a good fit for a cloud data management scenario.
%  Add how B-trees would make a better fit for remote storage with out method.

\inlinesection{Write Amplification.}
LSM-trees exhibit write amplification because data is written multiple times as it moves through the tree's storage hierarchy \cite{kuszmaul2014fractal}.
Each write is first stored in memory, then flushed to disk as an SSTable, and repeatedly rewritten during compactions that merge overlapping files across levels. 
These repeated rewrites cause the total bytes written to disk to exceed the amount of user data written, resulting in write amplification.

\inlinesection{Low Read Performance.}
Essentially, \ac{LSMT} trade high write performance at the cost of low read performance.
This is useful for specific scenarios where writes dominate reads.
However, it makes \ac{LSMT} unsuitable for general-purpose \ac{DBMS} as they incur significantly higher lookup costs compared to a B-tree as shown in \cite{gorrod2017wiredtiger}.

When performing point lookups, the \ac{LSMT} checks the MemTable first and then each level on disk from top to bottom until it is found or not.
In use-cases, where we only lookup hot keys that are likely to be in memory, \ac{LSMT} can perform well.
However, such temporal locality is an assumption that we cannot make in a general-purpose system that needs to balance performance for all use-cases.
When looking up cold keys that are not in memory, \ac{LSMT} need to check multiple files on disk, leading to high read amplification.

To improve lookup performance, each SSTable has an in-memory Bloom filter to check if a key is present in the file before performing a search \cite{chang2008bigtable}.
However, Bloom filters come with other problems. 
For one, it can yield false positives.
Secondly, the larger the data set they are addressing, the larger the Bloom filter needs to be, inflating the memory footprint of \ac{LSMT}.

Most importantly though, Bloom filters cannot handle range queries.
For range queries, all SSTables across levels must be checked. 
While there is an effort to improve range query performance in \ac{LSMT} \cite{zhong2021remix}, they are not designed for efficient range queries, as range data is scattered across the tree \cite{sarkar2022lsmt}.

\inlinesection{Summary.}
Overall, both \ac{LSMT} and B-trees are efficient data structures, but built for different scenarios.
This update/query trade-off has been well studied in literature \cite{brodal2003lower}.
In this thesis we focus on general-purpose database systems, which require balanced performance characteristics across-the-board.
For such a system, B-trees are the superior data structure.
We therefore investigate how to improve B-trees to close the gap in write performance to \ac{LSMT} while retaining their superior read performance.


\section{B$^\epsilon$-trees}
\label{sec:related-work-bepsilontrees}
\textbf{Basic Structure.}
B$^\epsilon$-trees \cite{bender2015epsilon} are a write-optimized variant of B-trees.
Each internal node has a buffer to temporarily encode incoming updates as messages.
When a buffer is full, messages are flushed to the appropriate child node.
When messages reach a leaf node, they are applied to the respective leaf.
Deletes are handled as tombstone messages that mark a key as deleted. Only when the message reaches the leaf, the key-value pair is removed from the leaf.
Each message encodes a timestamp to ensure that the updates are applied in the right order.

The $\epsilon$, which is a value between 0 and 1, refers to the tunable parameter that controls the size of the buffers in relation to the node size.
Given a page size $B$, it determines how much of its space is used for storing pivots ($B^\epsilon$) versus buffering updates ($B - B^\epsilon$).
Choosing a larger $\epsilon$ increases the space for keys and pointers, improving read performance similar to a B-tree, while a smaller $\epsilon$ increases the buffer size, enhancing write performance similar to a buffered repository tree \cite{buchsbaum2000external}.

\inlinesection{Mitigation of Write Amplification.}
This design allows B$^\epsilon$-trees to batch updates, reducing the number of \ac{IO} operations and improving write performance while maintaining comparable read performance to B-trees.
A benefit of using a top-down approach to propagate updates is that it primarily writes to higher levels of the tree which are more frequently accessed and thus more likely to be cached in memory.
Alongside with a good eviction strategy, this can effectively reduce number of write operations to external storage.
Another effect of this design it that is allows for large node sizes.
For one, we need larger node sizes to accommodate the buffers and maintain a high fanout.
But more importantly, batching updates mitigates write amplification.
At the time of reaching a leaf node to apply updates, many updates have accumulated and can be applied at once. 
A leaf node will not be rewritten for individual updates. 
Therefore, the larger node sizes are less problematic in B$^\epsilon$-trees, since they do not incur as much write amplification as in B-trees.

\inlinesection{Read Overhead.}
Messages are usually binary search trees like a red-black tree to allow efficient searching within the buffer.
When searching for a key, the tree is traversed from the root to the leaf, checking each buffer along the path for messages that belong to the key.
This ensures that the most recent updates are considered during the search.
However, this also means that two searches are required per node: one for the pointer to the child node and one for messages in the buffer.
This introduces some overhead for read operations compared to B-trees.

On the other hand, B$^\epsilon$-trees can achieve faster scans, because larger node sizes are more attractive in this design, better utilizing the bandwidth of external storage.

\inlinesection{Concurrency Limitation.}
Since updates are propagated top-down, we introduce contention on higher levels of the tree.
However, higher levels of the tree are more frequently accessed to locate entries.
When they are written to, this blocks a large amount of nodes below.
This is especially problematic for the root node, which needs to be accessed by every operation in the tree, limiting concurrency in the system significantly.

\inlinesection{Summary.}
While B$^\epsilon$-trees have been shown to effectively mitigate write amplification in a single-threaded scenario, they significantly limit concurrency in the data structure.
A characteristic that makes B$^\epsilon$-trees unsuitable for high-performance database systems.
In this thesis, we aim to reduce write amplification in B-trees while retaining high concurrency.


\section{Bw-trees}
\label{sec:related-work-bwtrees}
\inlinesection{Basic Structure.}
The Bw-tree \cite{levandoski2013bw} is a B-tree variant optimized for modern hardware.
It introduces a latch-free design, leveraging atomic compare-and-swap operations to ensure consistency without traditional locking mechanisms.
They employ out-of-place updates, where deltas are prepended to nodes as linked lists instead of modifying them in place.
This avoids cache invalidation, enabling higher concurrency in the tree.
To update the delta chain, they use atomic \ac{CAS} operations to allow latch-free updates.
The delta chain of a node is eventually consolidated, by creating a new node that applies the deltas to the base node.
Outdated base nodes are reclaimed by a garbage collector.
Additionally, they employ a log-structured store that migrates nodes to contiguous storage locations.
While they specifically target flash-based storage, the design principles apply to other storage media as well.

\inlinesection{Mitigation of Write Amplification.}
When a page with a delta-chain is flushed to external storage, only the new deltas need to be written, not the entire page.
This effectively reduces write amplification, as they only write the changes instead of the whole page.
The deltas of several pages can be consolidated in memory, allowing to batch writes to external storage.
Only when creating new pages during consolidation, they need to write the entire page.
In that case, the node has experienced sufficient modifications that justify writing the entire page.

\inlinesection{Read Overhead.}
The delta chain needs to be traversed for every single node on the read path to a leaf, introducing overhead for every lookup.
While the goal is to keep cache lines valid, applying deltas out-of-place and traversing a linked list of deltas per node, pollutes the caches of every core.
When loading a page from external storage, the entire delta chain needs to be read and applied to reconstruct the current state of the node.
This requires multiple random read operations on storage.

\inlinesection{High Coupling.}
This design introduces invasive changes to the B-tree's implementation, requiring a change in lookup and update logic as well as a consolidation mechanism and garbage collection. 
Most importantly, it heavily couples the cache management layer with the indexing layer.
For example, the indirection via the mapping table from \ac{PID}s to physical addresses becomes a requirement to implement the \ac{CAS} logic to update the delta chain.
The data structure needs to be aware of the storage layer and their implementation details to implement this logic.
This makes changes to the caching layer difficult. 
For example, pointer swizzling \cite{graefe2014memory} would be infeasible with this design. 
Everytime a delta is prepended to a node, the swizzled pointers would become invalid.
Updating each pointer to the new root of the delta chain would require updating all outdated pointers in the tree.
However, pointer swizzling is a common technique for disk-based database systems \cite{leis2018leanstore} to compete with in-memory database systems.
Therefore, we want to keep each layer transparent to the other, allowing independent optimizations.

\inlinesection{Summary.}
Overall, the Bw-tree presents a novel approach to reduce write amplification in B-trees and we take notes for our own design.
However, Wang et al. showed in their paper "Building a Bw-tree Takes More Than Just Buzz Words" \cite{wang2018bwtree} that the Bw-tree's performance is actually not competitive with traditional B-trees using optimistic lock coupling \cite{leis2019optimistic}.
In our approach, we aim to introduce a small overhead when loading a page from external storage, not for every read operation in memory.
Additionally, we want to keep the changes to the B-tree minimal, introducing a lightweight layer between the data structure and the storage manager that buffers and batches updates.   

\section{Write-Optimized B-trees}
\label{sec:related-work-wo-btrees}
Graefe proposes "Write-Optimized B-trees" \cite{graefe2004write} to address the write efficiency gap between log-structured file systems and the B-tree.
Log-structured data structures write large, sequential chunks of data to disk, making optimal use of the available bandwidth.
B-trees, on the other hand, perform many small, random writes by writing individual pages.
To improve write efficiency in B-trees, Graefe proposes to batch multiple dirty nodes and write them to disk in a single, large write operation.

The buffer manager can invoke such page migrations for the chosen dirty pages.
By introducing logical fence keys, the pages can be written to arbitrary locations, without requiring an update to the sibling pointers.
Since page migrations are optional, the B-tree can still decide to update pages in place if that is more efficient.

This work provides an approach to get the log-structured style of writing in a B-tree, without changing the data structure itself.
It supports the effort of improving write performance in B-trees, while retaining their high read performance and concurrency.
While this approach improves write performance by batching multiple pages, in this thesis we address the write amplification caused by individual page and in-place updates.
We avoid writing individual pages as a whole, by deferring updates and buffering them in batches instead.

\section{Decoupling Storage from In-Memory Data Structures}
\label{sec:related-work-fineline}
Sauer et al. \cite{sauer2018fineline} propose FineLine, a storage manager that decouples in-memory data structures from persistence concerns in a database system.
In-memory data structures are never propagated to disk.
Instead, FineLine merely uses the log for persistence.
When an in-memory data structure (for example a B-tree) causes a cache miss, FineLine reconstructs its state by replaying the log of operations from disk.
When a transaction commits, its log of changes to the data structures is flushed to disk in a sequential manner, allowing for high write performance.
To improve read performance for reconstruction, FineLine employs a compaction mechanism that merges partitions of the log, similar to \ac{LSMT}.

Since all flushes to disk are sequential, write amplification is small.
Some write amplification is introduced by the compaction process.
Also, traditional systems need to write both, the data structure pages and the \ac{WAL} to disk.
In FineLine, the log is the only component that requires persistence.
This reduces random writes in the system further.

FineLine also allows for a graceful recovery mechanism, as the log can be replayed when the in-memory state is lost (e.g., after a crash) in a similar way that a cache miss is handled.

One drawback of this approach is the high read amplification when reconstructing data structures from the log.
Every time a node is reconstructed, the log needs to be scanned for all entries that affect the node.
Due to compaction, this search can be efficient.
However, assume a high update rate on a certain data structure node.
Many partitions of the log may contain updates for this node, requiring multiple reads from disk to reconstruct the node.

% TODO: "Two is Better Than One: The Case for 2-Tree for Skewed Data Sets" https://www.cidrdb.org/cidr2023/papers/p57-zhou.pdf

\section{Transparent Compression}
\label{sec:related-work-compression}
Qiao et al. \cite{qiao2022compression} propose a hardware-based approach to reduce write amplification in B-trees.
They use transparent compression, a hardware feature of some modern storage devices that offer lossless data compression transparent to the host.
The storage device compresses data before writing it to the physical medium.
When a page is empty, no data is written at all.
When a page is partially filled, only the actual data is written, not the empty space.
This can be used to reduce write amplification in B-trees, as it allows for sparse data structures which do not actually waste space on the storage device.

In their approach, Qiao et al. apply out-of-place updates like previous approaches.
Each node's page is followed by a modification log that records changes to the node.
In contrast to previous approaches of out-of-place updates, they do not need to collect updates to a node across storage but instead, they can perform a single read operation to load the node and its modification log.
When loading the node from storage, they apply the modifications in the log to reconstruct the current state of the node.
When a node is modified, they append the modification to the log instead of rewriting the entire page.
When a node is flushed to storage, they obtain the delta, and decide whether they invoke the page modification logging which is appended to the page or whether they write the entire page in-place.

This approach reduces write amplification in B-trees, as they avoid rewriting entire pages for small updates.
However, for small deltas they still need to perform an \ac{IO} operation to write the modification log to storage.
The purpose of reducing write amplification is not only to reduce the amount of data written, but primarily to reduce the frequency of writing data by batching updates.
We want to avoid writing at all for small updates.

A benefit of this approach is that it does not require invasive changes to the B-tree structure itself.
In memory, the B-tree remains the same and only the storage manager needs to be aware of the modification log.
This only requires overhead at the point of loading and unloading a page to and from external storage.
A similar approach is taken in this thesis.
However, this approach relies on hardware-based compression which is not widely available.
We aim to provide a software-based solution that can be used on any storage device.

% \section{Partitioned B-trees}
% \section{Blink-trees} not relevant, just a concurrency mechanism
% \section{Fractal Trees} same as BepsilonTrees
% \section{Buffer Trees} covered by BepsilonTrees
% \section{In-Memory Data Structures} not relevant as we scop disk-based systems, see https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrees-are-back.pdf
% \section{Other B-tree Optimizations}
    